{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Covid-19 Forecasting using an RNN"},{"metadata":{},"cell_type":"markdown","source":"The goal of this notebook is very simple: Generate additional features from the Covid19-global-forecasting dataset and feed it into an RNN. The RNN will take as inputs:\n*     number of cases for 13 days\n*     number of fatalities for 13 days\n\nas outputs:\n*    number of cases for the 14th day\n*    number of fatalities for the 14th day"},{"metadata":{},"cell_type":"markdown","source":"* V5: Submission pipeline fixed - score: 3.09681\n* V6: New RNN architecture with two separate branches for each output - score: 2.25901\n* V8: Add a post-processing step checking if the model's output is equal or greater the previous value - score: 2.29932\n* V9: Change the MSE losses to RMSLE - score: 1.43247\n* V11: Change the outputs' activation fucntions from linear to ReLU - score: 1.26594\n* V12: Use a 2-week period for predictions instead of 1. Replace the SimpleRNN layers with LSTM layers - score: 1.14070\n* V13: Fix bug in the cell 4 (flagged by @jeremyoudin) which made the dataset much larger due to duplicates and also created a leakage between the training and validation sets. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport os\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom sklearn.utils import shuffle\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nfrom datetime import timedelta\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = gpd.read_file(\"/kaggle/input/covid19-global-forecasting-week-2/train.csv\")\ntrain_df[\"Country_Region\"] = [country_name.replace(\"'\",\"\") for country_name in train_df[\"Country_Region\"]]\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I create a new dataframe where I will only store 6-day trends for each location with the resulting numbers on the 7th day. The time periods extracted do not overlap on purpose."},{"metadata":{"trusted":true},"cell_type":"code","source":"trend_df = pd.DataFrame(columns={\"infection_trend\",\"fatality_trend\",\"expected_cases\",\"expected_fatalities\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Just getting rid of the first days to have a multiple of 14\n#Makes it easier to generate the sequences\ntrain_df = train_df.query(\"Date>'2020-01-22'and Date<='2020-03-18'\")\ndays_in_sequence = 14\n\nwith tqdm(total=len(list(train_df.Country_Region.unique()))) as pbar:\n    for country in train_df.Country_Region.unique():\n#         import pdb; pdb.set_trace()\n        for province in train_df.query(\"Country_Region=='\"+country+\"'\").Province_State.unique():\n            province_df = train_df.query(\"Country_Region=='\"+country+\"' and Province_State=='\"+province+\"'\")\n            for i in range(0,len(province_df)-days_in_sequence,days_in_sequence):\n\n                infection_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].ConfirmedCases.values]\n                fatality_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].Fatalities.values]\n                expected_cases = float(province_df.iloc[i+days_in_sequence].ConfirmedCases)\n                expected_fatalities = float(province_df.iloc[i+days_in_sequence].Fatalities)\n                                            \n                trend_df = trend_df.append({\"infection_trend\":infection_trend,\n                                 \"fatality_trend\":fatality_trend,\n                                 \"expected_cases\":expected_cases,\n                                 \"expected_fatalities\":expected_fatalities},ignore_index=True)\n        pbar.update(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Shuffling the dataframe to make sure we have a bit of everything in our training and validation set."},{"metadata":{"trusted":true},"cell_type":"code","source":"trend_df[\"input\"] = [np.asarray([trends[\"infection_trend\"],trends[\"fatality_trend\"]]) for idx,trends in trend_df.iterrows()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trend_df = shuffle(trend_df)\ntrend_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only keeping 2000 sequences where the number of cases stays at 0, as there were way too many of these samples in our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"i=0\ny=0\ntemp_df = pd.DataFrame(columns={\"infection_trend\",\"fatality_trend\",\"expected_cases\",\"expected_fatalities\",\"input\"})\nfor idx,row in trend_df.iterrows():\n    if sum(row.infection_trend)>0:\n        temp_df = temp_df.append(row)\n    else:\n        if i<100:\n            temp_df = temp_df.append(row)\n            i+=1\ntrend_df = temp_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trend_df[:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting my dataset - 90% for training and 10% for validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"sequence_length = 13\ntraining_percentage = 0.9","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_item_count = int(len(trend_df)*training_percentage)\nvalidation_item_count = len(trend_df)-int(len(trend_df)*training_percentage)\ntraining_df = trend_df[:training_item_count]\nvalidation_df = trend_df[training_item_count:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = np.asarray(np.reshape(np.asarray([np.asarray(x) for x in training_df[\"input\"]]),(training_item_count,2,sequence_length))).astype(np.float32)\nY_cases_train = np.asarray([np.asarray(x) for x in training_df[\"expected_cases\"]]).astype(np.float32)\nY_fatalities_train = np.asarray([np.asarray(x) for x in training_df[\"expected_fatalities\"]]).astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = np.asarray(np.reshape(np.asarray([np.asarray(x) for x in validation_df[\"input\"]]),(validation_item_count,2,sequence_length))).astype(np.float32)\nY_cases_test = np.asarray([np.asarray(x) for x in validation_df[\"expected_cases\"]]).astype(np.float32)\nY_fatalities_test = np.asarray([np.asarray(x) for x in validation_df[\"expected_fatalities\"]]).astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build the model"},{"metadata":{},"cell_type":"markdown","source":"The model is very simple in terms of architecture. The only difference from what could traditionally be seen is that it has two outputs so we can have two different losses (one for the expected number of cases and for the expected number of fatalities)."},{"metadata":{"trusted":true},"cell_type":"code","source":"\ninput_layer = Input(shape=(2,sequence_length))\nmain_rnn_layer = layers.LSTM(128, return_sequences=True, recurrent_dropout=0.2)(input_layer)\n\nrnn_c = layers.LSTM(64)(main_rnn_layer)\nrnn_f = layers.LSTM(64)(main_rnn_layer)\n\ndense_c = layers.Dense(256)(rnn_c)\ndropout_c = layers.Dropout(0.3)(dense_c)\n\ndense_f = layers.Dense(256)(rnn_f)\ndropout_f = layers.Dropout(0.3)(dense_f)\n\ncases = layers.Dense(1, activation=\"relu\",name=\"cases\")(dropout_c)\nfatalities = layers.Dense(1, activation=\"relu\", name=\"fatalities\")(dropout_f)\nmodel = Model(input_layer, [cases,fatalities])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=4, verbose=1, factor=0.7),\n             EarlyStopping(monitor='val_loss', patience=20),\n             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\nmodel.compile(loss=[tf.keras.losses.MeanSquaredLogarithmicError(),tf.keras.losses.MeanSquaredLogarithmicError()], optimizer='adam')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, [Y_cases_train, Y_fatalities_train], \n          epochs = 200, \n          batch_size = 16, \n          validation_data=(X_test,  [Y_cases_test, Y_fatalities_test]), \n          callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Performance during training"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Loss over epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['cases_loss'])\nplt.plot(history.history['val_cases_loss'])\nplt.title('Loss over epochs for the number of cases')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['fatalities_loss'])\nplt.plot(history.history['val_fatalities_loss'])\nplt.title('Loss over epochs for the number of fatalities')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generate predictions using the model"},{"metadata":{},"cell_type":"markdown","source":"We can quickly check the quality of the predictions... One thing is clear, there is room for improvement!"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights(\"best_model.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_limit = 30\nfor inputs, pred_cases, exp_cases, pred_fatalities, exp_fatalities in zip(X_test,predictions[0][:display_limit], Y_cases_test[:display_limit], predictions[1][:display_limit], Y_fatalities_test[:display_limit]):\n    print(\"================================================\")\n    print(inputs)\n    print(\"Expected cases:\", exp_cases, \" Prediction:\", pred_cases[0], \"Expected fatalities:\", exp_fatalities, \" Prediction:\", pred_fatalities[0] )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Apply the model on this existing data"},{"metadata":{},"cell_type":"markdown","source":"The following functions will be used to get the 6 previous days from a given date, predict the number of cases and fatalities, before iterating again. Therefore, it will use the prediction for the next day as part of the data for the one afterwards."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Will retrieve the number of cases and fatalities for the past 6 days from the given date\ndef build_inputs_for_date(country, province, date, df):\n    start_date = date - timedelta(days=13)\n    end_date = date - timedelta(days=1)\n    \n    str_start_date = start_date.strftime(\"%Y-%m-%d\")\n    str_end_date = end_date.strftime(\"%Y-%m-%d\")\n    df = df.query(\"Country_Region=='\"+country+\"' and Province_State=='\"+province+\"' and Date>='\"+str_start_date+\"' and Date<='\"+str_end_date+\"'\")\n    input_data = np.reshape(np.asarray([df[\"ConfirmedCases\"],df[\"Fatalities\"]]),(2,sequence_length)).astype(np.float32)\n    \n    return input_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Take a dataframe in input, will do the predictions and return the dataframe with extra rows\n#containing the predictions\ndef predict_for_region(country, province, df):\n    begin_prediction = \"2020-03-19\"\n    start_date = datetime.strptime(begin_prediction,\"%Y-%m-%d\")\n    end_prediction = \"2020-04-30\"\n    end_date = datetime.strptime(end_prediction,\"%Y-%m-%d\")\n    \n    date_list = [start_date + timedelta(days=x) for x in range((end_date-start_date).days+1)]\n    for date in date_list:\n        input_data = build_inputs_for_date(country, province, date, df)\n        result = model.predict(np.array([input_data]))\n        \n        #just ensuring that the outputs is above 0\n        # or higher than the previous counts\n        #Get the absolute value for the number of cases\n        \n        result[0] = np.round(result[0])\n        if result[0]<input_data[0][-1]:\n            result[0]=np.array([[input_data[0][-1]]])\n        \n        result[1] = np.round(result[1])\n        if result[1]<input_data[1][-1]:\n            result[1]=np.array([[input_data[1][-1]]])\n            \n        df = df.append({\"Country_Region\":country, \n                        \"Province_State\":province, \n                        \"Date\":date.strftime(\"%Y-%m-%d\"), \n                        \"ConfirmedCases\":round(result[0][0][0]),\t\n                        \"Fatalities\":round(result[1][0][0])},ignore_index=True)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"copy_df = train_df\nwith tqdm(total=len(list(copy_df.Country_Region.unique()))) as pbar:\n    for country in copy_df.Country_Region.unique():\n        for province in copy_df.query(\"Country_Region=='\"+country+\"'\").Province_State.unique():\n            copy_df = predict_for_region(country, province, copy_df)\n        pbar.update(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Example"},{"metadata":{"trusted":true},"cell_type":"code","source":"copy_df.query(\"Country_Region=='France' and Province_State=='French Guiana' and Date>'2020-03-10'\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = gpd.read_file(\"/kaggle/input/covid19-global-forecasting-week-2/test.csv\")\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just need to do this little trick to extract the relevant date and the forecastId and add that to the submission file."},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = pd.DataFrame(columns=[\"ForecastId\",\"ConfirmedCases\",\"Fatalities\"])\nfor idx, row in test_df.iterrows():\n    #Had to remove single quotes because of countries like Cote D'Ivoire for example\n    country_region = row.Country_Region.replace(\"'\",\"\").strip(\" \")\n    province_state = row.Province_State.replace(\"'\",\"\").strip(\" \")\n    item = copy_df.query(\"Country_Region=='\"+country_region+\"' and Province_State=='\"+province_state+\"' and Date=='\"+row.Date+\"'\")\n    submission_df = submission_df.append({\"ForecastId\":row.ForecastId,\n                                          \"ConfirmedCases\":int(item.ConfirmedCases.values[0]),\n                                          \"Fatalities\":int(item.Fatalities.values[0])},\n                                         ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.sample(20)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}